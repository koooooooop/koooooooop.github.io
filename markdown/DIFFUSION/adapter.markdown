# 基于Adapter的Diffusion漫画多帧一致性优化技术综述

近年来，扩散模型在图像生成领域取得了显著突破，特别是在根据小说文本生成连贯漫画或长视频方面表现突出。然而，多帧图像（连环画/漫画）生成中存在角色、场景、风格等一致性难题。在传统生成技术中，不同场景的同一角色往往会出现面貌或服饰上的不一致。为了提高连贯性，ByteDance提出的**StoryDiffusion**等模型引入了“一致性自注意力”（Consistent Self-Attention）和语义运动预测（Semantic Motion Predictor）等新机制，能够在无训练或弱训练情况下保持生成序列中角色和场景高度一致。下图为StoryDiffusion生成的示例，其人物形象和服饰风格在不同帧中保持统一：

![ByteDance StoryDiffusion示例](./DIFFUSION.png)

&#x20;*图：ByteDance StoryDiffusion示例。左侧为《丛林历险》场景，右侧为“The Moon Exploration by Lecun”场景。可见角色面貌和服饰在各帧中保持一致。*

上述实例显示，通过特殊的注意力机制，生成模型能够在多帧输出中保持主题连贯性。本文基于已有的“Diffusion模型在AI小说生成漫画中的深度技术综述”，重点探讨在Stable Diffusion等U-Net架构中**引入Adapter模块**以优化多帧一致性的问题。我们将分析Adapter在注意力与非注意力层的插入方式、多种主流一致性优化技术、专用Adapter设计策略以及完整的训练推理流程，并在末章提出\*\*“Adapter优化Diffusion一致性的系统性方案”\*\*。

## Adapter模块在U-Net注意力块中的插入技术

Adapter模块本质上是一种轻量级可训练子网络，可插入到预训练模型的特定层中，用以注入新能力而无需大规模调整原始权重。对于Stable Diffusion的U-Net，其核心由多个Transformer块（含自注意力 Self-Attention、交叉注意力 Cross-Attention 和前馈层FFN）组成。Adapter插入时机和位置选择对于下游效果至关重要。常见的几种插入方式包括：

* **串行插入（前插/后插）**：将Adapter作为顺序模块插入到原有模块之前或之后。例如，可以在注意力计算输入之前（前插）或输出之后（后插）插入一个小型MLP或卷积块。Xiang等人研究表明，**Adapter的输入位置**是性能关键因素，往往在跨注意力（Cross-Attention）模块之后插入Adapter性能最佳。具体而言，可配置的插入点包括`attn1i/attn1o`（第一层自注意力输入/输出）、`attn2i/attn2o`（第二层注意力输入/输出）、`attn2c`（交叉注意力的上下文输入）、`ffni/ffno`（前馈层输入/输出）等。

* **并行插入（Parallel）**：在原模块的输出中加入Adapter的并行分支，通常使用加权或门控融合。Lialin等人分析指出，将Adapter并行于主路径（尤其是并行于FFN）往往优于串行插入，并行Adapter经过缩放后能更好地提升性能。具体可实现方式包括在注意力层或前馈层输出后，将Adapter输出与原始输出相加或通过可学习门控相加，从而**部分取代**原有信息。

* **融合方式**：无论串行还是并行插入，Adapter输出需要与原注意力路径融合。常见的融合方法有简单相加（Residual）、元素级乘法（如可学习缩放因子）或门控融合等。例如，LoRA通过低秩矩阵调整原权重，等价于在前馈或注意力层的输出上加上新矩阵。在并行模式下，Adapter常带缩放参数，将其输出与原输出“线性组合”。下式展示了一种典型的并行融合形式：$Z' = Z_{\text{orig}} + \lambda\,f_{\text{adapter}}(X) \,,$其中\$Z\_{\text{orig}}\$为原模块输出，\$f\_{\text{adapter}}\$为Adapter网络，\$\lambda\$为缩放权重。通过学习\$\lambda\$或加入非线性激活，可控制Adapter对注意力计算的影响。

综上，在U-Net注意力块中插入Adapter时，需要在保持原始权重冻结的前提下选择合适的插入位置和结构。实践中，研究者常在Transformer的自注意力或交叉注意力层前后插入Adapter，并考虑使用并行分支来提高对原图的影响。如下表所示，现有工作已尝试将Adapter放在自注意力层、交叉注意力层和FFN层的不同位置（输入/输出/上下文）进行微调：

* 插入位置示例：`Attn1i`(第1自注意力输入)、`Attn1o`(第1自注意力输出)、`Attn2i`/`Attn2o`(第2自注意力/交叉注意力)、`Attn2c`(交叉注意力上下文输入)、`FFNi`/`FFNo`(前馈层)。
* 功能形式：常见包括小型全连接网络（MLP）、卷积层或可学习的低秩矩阵（LoRA风格）等。
* 串行与并行：Lialin等人的研究表明，相比串行接入，并行添加Adapter（尤其并行于FFN）的效果更优。

## Adapter在其他非注意力层的插入策略及比较

除了注意力块，U-Net中还有大量ResBlock卷积结构和Skip Connection等可插入Adapter的位置。以下是几种可行的插入策略及其特点：

* **ResNet块（ResBlock）中插入**：在U-Net的ResBlock中，可将Adapter插入到卷积层之后或加入跳跃连接路径上。例如，可在ResBlock的输出通道后接入一个小型卷积或MLP，或者并行地提取特征后相加。这样做可以直接对局部特征进行额外调制，适用于关注纹理和风格信息的一致性优化。然而，这通常不改变全局结构信息。适当的ResBlock插入可以改进生成细节，但对整体布局一致性的影响可能有限。

* **交叉注意力层中插入**：对于文本到图像（Text-to-Image）任务，U-Net中的交叉注意力层（Cross-Attention）接收文本条件。可在此处插入Adapter来引入额外条件。IP-Adapter即是一例，其在交叉注意力中\*\*共享原有查询（Q）**的同时**增加了新的键（K）和值（V）\*\*用于图像提示信息。具体而言，原交叉注意力计算式为\$Softmax(QK^T)V\$，IP-Adapter将其扩展为：

  $$
  \text{Attn}'(Q,[K_{\text{text}},K_{\text{img}}],[V_{\text{text}},V_{\text{img}}])
  =\text{Softmax}\Bigl(\frac{Q\,[K_{\text{text}},K_{\text{img}}]^T}{\sqrt{d}}\Bigr)\,[V_{\text{text}},V_{\text{img}}]\,,
  $$

  其中附加的\$K\_{\text{img}},V\_{\text{img}}\$来自图像提示路径，共享\$Q\$来自文本条件。这种插入方法相当于在不修改Transformer架构的情况下，给交叉注意力注入了新的信息源，在语义对齐和一致性控制方面很灵活。

* **控制网（ControlNet）适配**：对于引入外部控制条件（如边缘图、姿势图）的问题，可以采用类似Ctrl-Adapter的方法。Ctrl-Adapter框架中，研究者为不同的生成模型训练Adapter层，将预训练的ControlNet特征融合入主模型，同时冻结ControlNet和U-Net参数。其设计包括时空模块，专门处理视频的时间一致性。由此可见，通过训练Adapter层融合第三方输入（如ControlNet或参考图像），可以拓展Diffusion模型的能力而不需重训原模型。

* **对比分析**：总体上，ResBlock插入更多影响细节处理和特征变换，而在Cross-Attention或ControlNet路径插入，则直接引入新的条件信息或全局结构。ResBlock Adapter通常参数更少、作用局限于特征层面；Cross-Att Adapter能够精准地控制全局语义，对保持角色姿态与环境一致性更有力。在众多实验中表明，通过替换或并行插入交叉注意力组件，可以显著提高生成结果的主题连贯度。

## 主流注意力一致性优化技术综述

针对多帧或长视频生成中的一致性问题，学术界和产业界提出了诸多注意力机制的改进方案，主要包括以下几类方法：

* **跨帧注意力（Cross-frame Attention）**：在生成过程中引入跨帧信息交互，以强化相邻帧之间的关联。比如在视频生成中，FancyVideo 提出**跨帧文本引导模块**（Cross-frame Textual Guidance Module, CTGM），在原有的空间交叉注意力前后插入三子模块：时序信息注入器（Temporal Information Injector, TII）、时序关联优化器（Temporal Affinity Refiner, TAR）和时序特征增强器（Temporal Feature Booster, TFB），分别在时间维度上注入帧级条件、精化文本嵌入与帧特征的关联矩阵、强化时序一致性。CTGM通过为每一帧构建特定的文本条件，并调整其与其他帧的注意力关系，显著提升了视频生成的连贯性。类似地，I2V-Adapter通过将参考图像的内容跨帧传播来保持身份一致性：该方法使用跨帧注意力将未加噪的参考图片信息传递到后续帧，从而在不改变预训练模型的前提下维护了人物身份一致性。

* **共享Q/K策略**：另一些方法尝试让不同帧使用相同或相关的Query/Key向量，以对齐注意力计算。例如，IP-Adapter对共享Query的方式已在前文提及。在面向视频编辑的研究中，也有将自注意力转换为跨帧注意力的尝试，即将当前帧的查询与邻帧的键值进行配对。Object-Centric Diffusion for Video Editing的研究指出，将跨帧注意力（即从相邻帧抽取K/V）的引入能够显著提高时序一致性。他们总结：“引入跨帧注意力可以极大地增强时间一致性，尤其是当涉及更多帧时”。可见，通过共享或重新组合Query/Key，可以让模型在注意力层面捕获帧间关系。

* **面向对象的注意力（Object-centric Attention）**：部分方法聚焦于图像中的关键对象，通过稀疏化背景或对象分片来减少冗余计算，并集中注意力在目标上。例如，Object-Centric 3D-ToMe等研究利用3D逐渐合并（splatting）等技术，将冗余背景token融合，专注于保持主体一致性。这些方法在保持时序连贯性的同时，也显著加速了生成过程。

* **场景/身份条件注入**：利用显式条件（如参考图像或场景标签）来指导注意力。**Compositional Guidance**（如Visual Composer中提出的对象级组合指引）通过将提示词与分割出的对象级特征混合，保证生成图像中目标的语义正确及身份一致。具体而言，研究者在推理时对每个对象使用交叉注意力，将对象嵌入与文本引导结合，从而改善了角色身份和布局正确性。**Reference Attention**则广泛用于保持一致性：例如Spatially Conditioned Diffusion模型中采用参考注意力机制，将参考图像特征仅与自身互动，而目标帧可以查询参考图像的特征，从而在生成过程中强制保持外观不变。实验结果表明，引入参考注意力后的模型在细节保存方面表现更好，相关度指标显著提升。

这些方法各有侧重点：跨帧注意力与共享Q/K直接增强时间相关性，对动态生成有效；面向对象的方法兼顾效率与重点；显式条件注入则通过额外的先验（如人物参考图像）来控制身份和场景一致性。以上策略提供了多种角度的思路，为后续Adapter设计奠定参考基础。

## 专用于一致性优化的Adapter设计策略

在为多帧一致性设计专用Adapter时，需要考虑参数共享、局部/全局信息融合及引导机制等因素：

* **参数共享与分离**：可根据任务需求决定是否在不同帧之间共享Adapter参数。一种思路是让所有时间帧共用一套Adapter权重，从结构上统一角色特征学习；另一种则为每一帧设计独立Adapter，以捕捉局部变化。共享参数有助于模型从数据中学习到跨帧不变的部分（如固定角色特征），而独立参数则给模型更大灵活度以适应不同帧环境。Zhou等人在StoryDiffusion实验中还探索了向生成过程引入**外部控制ID**的方案，通过提供身份图片引导生成，使角色在不同帧中保持一致。这类似于为Adapter提供额外输入，通过对其输出进行适当融合来强化一致性。

* **局部与全局融合**：一致性优化应兼顾局部细节与全局布局。一种设计是让Adapter同时处理局部特征和全局信息，例如将某些Adapter聚焦于当前帧的细节（局部注意力），另一些Adapter负责引入跨帧上下文（全局注意力）。通过融合局部和全局注意力，模型既能保证每帧自身的清晰度，又能捕捉序列中的场景连贯性。例如，可以在U-Net的浅层使用更像常规模块处理局部纹理，而在深层或注意力层使用Adapter跨帧混合特征。

* **引导式注意力结构**：Adapter可以设计为引导注意力关注特定方面。比如使用人物身份特征、语义标签或位置掩码作为附加输入，引导Adapter专注于保持角色或物体不变。类似于ControlNet的方式，一个Adapter可以接收额外的条件输入（如目标对象的掩码或姿态），通过融合这个条件来调节注意力权重，实现对角色外观的一致性控制。实验证明，在StoryDiffusion中加入ID控制图像就能显著提升身份一致率，这为Adapter引入辅助条件提供了思路。

综上，对于一致性任务，可采取**共享参数+局部全局混合+条件引导**的设计原则，即在保证模型灵活性的同时，将全球一致性信息融入到注意力计算中。这些策略可结合实际应用需求调整Adapter结构和超参数设置。

## Adapter训练流程：数据准备、评估指标与损失函数

要训练一致性优化的Adapter，需要构建合适的数据集和评价指标，并设计专项损失函数。一般而言，完整的训练流程包括以下环节：

* **数据采集与标注**：需要具有多帧一致角色的图像数据。可以采用漫画、动画片段或视频数据集中的角色序列作为训练数据。每个训练样本应包含相同角色或场景的多帧图像集，最好附带相应的文本描述或时间戳，以指导帧间关系。对这些数据进行标注时，需要确认哪些帧对应同一角色或场景，可采用现有的人脸/物体跟踪算法自动标注身份ID。此外，如果设计了风格或结构要求，还可以对数据进行属性标注，如角色服装类别、场景标签等，以便用在条件输入和评价中。

* **采样策略**：在训练时，一种常见做法是按故事情节将长段文本拆分为多个短提示，并让模型同时生成多个相关图像帧。为适应Adapter训练，可在每次迭代中随机选取若干帧作为一组(batch)，保证同组内图像具有共同角色或主题。也可以采用滑动窗口机制，对长序列图片滑动抽取训练样本，使得Adapter在多样的跨帧距离上学习一致性。例如，Zhou等人将输入文本拆分并批量生成多帧图像，使Adapter能够在批量图像间构建连接。

* **一致性评估指标**：训练和验证时，需要衡量生成图像的连贯性。常用指标包括**LPIPS**（Learned Perceptual Image Patch Similarity）用于测量帧间的感知相似度、**CLIP-Image**相似度用于语义一致性评估，以及**身份一致率**（例如采用预训练人脸识别网络，计算同一角色在各帧中的嵌入距离或准确率）。文献中已有研究对多帧生成的评估多使用这些指标。例如，在空间条件扩散（SCD）模型的评测中，同时使用了PSNR、SSIM、LPIPS等图像质量指标，以及视频层面的FID-VID和FVD来衡量帧间一致性。综上，我们可以选取类似LPIPS/CLIP相似度评价生成帧与参考帧的视觉一致度，并统计诸如“同一ID识别正确率”来量化角色一致性。

* **损失函数**：针对一致性的损失通常包括风格一致性损失和结构约束损失等。风格损失可以采用Gram矩阵或感知损失，让多个帧的深层特征匹配，促进色彩与纹理上的统一；结构损失可引入对齐损失，如使用光流或关键点检测确保帧间几何结构连贯。具体实现时，可定义类似于视频生成中的时序损失，将相邻帧特征的差异最小化。此外，还可以针对人物区域或关键对象设计局部一致性损失，例如对相同角色的区域掩码进行L1/LPIPS损失，以强化身份统一。

总之，从数据到评估再到损失，需要一个**闭环**的流程：采集含多帧一致角色的图像集，按情节或时间采样训练样本；使用LPIPS、CLIP等指标评估生成连贯性；设计多重损失（风格+结构）以训练Adapter网络，确保优化目标与一致性直接相关。

## 推理阶段Adapter加载与融合方式

在推理过程中，将Adapter加载到预训练模型并与其融合也需要仔细考虑：

* **加载机制**：一般情况下，Adapter权重与预训练模型权重分开存储。在使用时，需要在U-Net相应模块中插入Adapter组件或替换部分计算流程。例如，将训练好的Adapter文件载入后插到每个Transformer层中。由于Adapter设计时假设原始模型参数冻结，推理时通常无需加载新的模型，只需将Adapter参数附加到U-Net即可。I2V-Adapter在示例中强调：其设计“在任何预训练模型都不做修改的前提下”工作，只需引入几个可学习参数即可。

* **参数冻结**：为了保留原模型的生成能力且避免过度拟合，推理时应冻结原有U-Net权重，仅激活Adapter参数。这样可以利用原模型的丰富先验，并仅通过Adapter的微调部分来调整输出。LORA等方法即采用这种策略：全程固定原始网络，只训练少量的低秩矩阵，最终将它们与原参数合并。类似地，Adapter推理时将原计算结果与Adapter计算结果融合即可，无需再次微调主模型。

* **融合路径**：最后需要确定Adapter输出如何与原始输出融合。常见的做法是简单相加（残差方式），将Adapter对特定特征的增量添加到U-Net的激活或注意力矩阵上。例如，在LORA中训练结束后，将\$\mathbf{W}\_A\mathbf{W}\_B\$的乘积直接加到原权重矩阵\$\mathbf{W}\$上，实现“部署时直接合并”。对于并行Adapter，可以将其输出通过可学习权重与主路径加权平均。总之，推理时原模型的走线不变，Adapter输出通过加法或其它融合层叠加上去，从而在生成过程中实时调整注意力或特征图，最终得到经过一致性优化的输出。

## Adapter优化Diffusion一致性的系统性方案分析

本章从理论和实证角度系统分析Adapter在Diffusion生成一致性优化中的作用原理及其与其他技术的对比优势，并给出具体的设计建议。

### 理论基础：注意力路径与一致性问题

Diffusion模型通过迭代去噪生成图像，U-Net中的注意力机制负责捕捉不同位置和模态（如文本提示）之间的关系。在多帧生成场景中，如果每帧的注意力计算相互独立，则难以保证角色或场景元素的全局一致性。多个研究指出，Diffusion模型**依赖注意力机制保持全局一致性**。当注意力机制缺乏跨帧信息时，生成过程往往只关注当前帧自身特征，导致帧间不连贯。Enhance-A-Video 的分析表明，在标准DiT（Video Diffusion）模型中，跨帧的注意力权重（即注意力矩阵的非对角元素）普遍远低于帧内权重。该研究总结：“跨帧注意力比对角（帧内）注意力弱得多，这会导致图像间出现突兀的转换”。因此，一致性问题**集中体现在Attention路径上**：如果不调整注意力计算，Diffusion模型很难自发地将帧间联系起来。综上，通过修改注意力结构（如插入Adapter或跨帧Attention）来引入帧间关联，是解决多帧一致性的核心思路。

### Adapter优化一致性的原理示意与数学说明

Adapter在注意力路径中的插入可以用以下数学形式来描述。首先，标准自注意力计算为：

$$
\text{Attn}(Q,K,V) = \text{Softmax}\Bigl(\frac{QK^\top}{\sqrt{d}}\Bigr)V\,,
$$

其中\$Q,K,V\$分别是查询、键、值矩阵。在Consistent Self-Attention（CSA）等方法中，Adapter将跨帧信息融入键值对，如Zhou等文献所述。假设我们考虑第\$i\$帧的注意力，将其与第\$j\$帧信息混合，则可以将\$K^i,V^i\$扩展为包括来自其他帧的键值：

$$
K'_i = [K^i; K^j],\quad V'_i=[V^i; V^j]\,.
$$

此时注意力输出为

$$
\text{CSA}(Q^i;K'_i,V'_i)= \text{Softmax}\Bigl(\frac{Q^i (K'_i)^\top}{\sqrt{d}}\Bigr) V'_i\,,
$$

它等价于对第\$i\$帧查询同时关注自身和其他帧的特征，从而在生成时引入帧间关联。若引入Adapter模块\$g(\cdot)\$到注意力路径，可以视作对查询或键值的增量调整。例如，采用并行相加方式时，可写为：

$$
Z_i' = Z_i + \lambda\,g(X_i)\,,
$$

其中\$Z\_i=\text{Attn}(Q^i,K^i,V^i)\$是原始输出，\$g(X\_i)\$为Adapter对第\$i\$帧特征\$X\_i\$计算的调整项，\$\lambda\$为可学习缩放因子。通过优化\$g(\cdot)\$和\$\lambda\$，模型学习到将跨帧相似性作为额外补偿注入生成过程。类似地，在交叉注意力中引入Adapter可以表示为改变注意力的键值对：如前文所述的IP-Adapter示例，其交叉注意力变为

$$
\text{Attn}'(Q,[K_{\text{text}},K_{\text{img}}],[V_{\text{text}},V_{\text{img}}])
= \text{Softmax}\Bigl(\frac{Q [K_{\text{text}},K_{\text{img}}]^\top}{\sqrt{d}}\Bigr)\,[V_{\text{text}},V_{\text{img}}]\,,
$$

相当于在原有文本键值外增加了图像提示信息。总之，Adapter优化在数学上就是对注意力计算中的\$Q,K,V\$进行增量修改，使得生成时包含更多跨帧或参考信息，从而提升一致性。

### 与其他一致性技术的对比优势

与直接修改U-Net架构或训练大规模视频模型相比，引入Adapter具有显著优势：

* **参数高效**：Adapter通常只占原模型参数很小的比例（如LORA方案额外参数不到0.75%），训练时计算开销小。相比之下，训练视频Diffusion需扩充模型（如3D卷积）或在每帧引入大量计算，成本很高。

* **训练/推理灵活**：Adapter可以在原有Stable Diffusion模型上**无训练或少量训练**地插入，零基础地利用已有模型生成一致图像（如StoryDiffusion零-shot模式）。推理时仅加载Adapter权重即可，不破坏原模型结构。而其他方法（如专门的Temporal UNet）通常需要重新训练整个模型。

* **可插拔性**：由于Adapter仅为附加模块，可根据需求随意选择插入层次和时机，无需改动原网络。例如，Consistent Self-Attention通过在自注意力处插入Adapter即可获得一致性提升；FancyVideo通过在交叉注意力路径外围加插CTGM子模块实现一致性增强。这种设计使得Adapter方案非常灵活，便于集成现有生成管线。

* **可控性强**：Adapter结构可融合额外控制信号（如身份图像、语义掩码），具备更好的可控生成能力。与仅依赖输入提示的方式相比，Adapter可在训练时引入明确的一致性损失和条件，从而更精细地调节生成结果。综合来看，Adapter在保留原模型性能的同时，通过少量可学习参数实现一致性优化，兼具灵活性和高效性，是一种优势明显的技术选型。

### 设计建议

基于以上分析，针对AI小说生成漫画任务，提出以下Adapter方案设计建议：

* **插入位置**：建议在U-Net的**中高层自注意力/交叉注意力模块后**插入Adapter，以便干扰较强的高级语义表示。Xiang等建议在交叉注意力后插入可获得最佳效果。并且，结合并行融合（Residual）可稳定控制输出强度。

* **结构形式**：可采用低秩分解或小型MLP作为Adapter函数，一方面参数量少，另一方面具备一定表达能力。如LORA那样的低秩矩阵或两层MLP都是可行的。对于多帧一致性，可考虑将Adapter设计为并行分支，并加入可学习的缩放门控。

* **条件引导**：若场景允许，给Adapter提供外部参考信息（如角色ID图像或场景布局）会有帮助。例如，可将控制网络（如角色参考图片）的特征作为Adapter输入，使其在生成时引导注意力关注相应角色。这样能进一步提高风格和身份的一致度。

* **参数共享**：对Adapter参数可共享可灵活调整。若数据中角色较为固定，建议在时间维度上共享Adapter参数，促进模型学习出全局不变特征；若场景变化较大，可以不共享，让不同帧的Adapter独立微调。

* **融合策略**：在实际部署中，可先冻结原网络，仅调整Adapter输出的权重系数（如缩放\$\lambda\$），以便在评估时控制一致性强弱。最后将Adapter输出与原输出相加，或者线性组合，以实现平滑过渡。为防止引入伪影，应在插值融合时加入平滑或条件判断。

总之，针对本任务的Adapter设计应注重在注意力层插入跨帧信息模块，结合条件引导与多层次融合，使生成的漫画序列在角色身份、服饰和场景布局上都能保持高度一致。

**参考文献**：Huang等对视频Diffusion的注意力分析、Zhou等StoryDiffusion方案、I2V-Adapter、Xiang等Diffusion微调工作、Lialin等Adapter并行策略、Visual Composer中的Compositional Guidance、Spatially Conditioned Diffusion的参考注意力等为本文提供了理论与实证支持。
