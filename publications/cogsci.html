<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Who Detects Better? A Comparative Study on Misinformation Detection | Zhipeng Zhou</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lora:wght@400;500;600;700&family=Open+Sans:wght@300;400;600;700&display=swap">
    <style>
        body {
            font-family: 'Open Sans', sans-serif;
            color: #333;
            line-height: 1.6;
            background-color: #f9f9f9;
        }
        h1, h2, h3, h4, h5, h6 {
            font-family: 'Lora', serif;
            color: #2c3e50;
        }
        .navbar {
            padding: 15px 0;
            background-color: #fff;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        .navbar-brand {
            font-size: 1.5rem;
            font-weight: 700;
            color: #2c3e50;
        }
        .nav-link {
            color: #555;
            font-weight: 600;
            margin-left: 15px;
            transition: color 0.3s ease;
        }
        .nav-link:hover {
            color: #4a6fa5;
        }
        .header-section {
            padding: 100px 0 60px;
            background-color: #fff;
        }
        .paper-title {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 20px;
            line-height: 1.3;
        }
        .authors {
            margin-bottom: 30px;
            font-size: 1.1rem;
        }
        .author {
            display: inline-block;
            margin-right: 15px;
        }
        .institution {
            font-size: 0.9rem;
            color: #666;
            margin-top: 10px;
        }
        .paper-links {
            margin: 25px 0;
        }
        .paper-link {
            display: inline-block;
            margin-right: 15px;
            font-weight: 600;
            padding: 8px 20px;
            color: #4a6fa5;
            border: 2px solid #4a6fa5;
            border-radius: 30px;
            transition: all 0.3s ease;
        }
        .paper-link:hover {
            background-color: #4a6fa5;
            color: #fff;
            text-decoration: none;
        }
        .content-section {
            padding: 60px 0;
            background-color: #fff;
        }
        .section-title {
            font-size: 1.8rem;
            font-weight: 700;
            margin-bottom: 25px;
            color: #2c3e50;
        }
        .abstract {
            background-color: #f8f9fa;
            padding: 25px;
            border-radius: 5px;
            margin-bottom: 30px;
        }
        .gallery {
            margin: 40px 0;
        }
        .gallery-image {
            width: 100%;
            border-radius: 5px;
            margin-bottom: 20px;
            box-shadow: 0 3px 10px rgba(0,0,0,0.1);
        }
        .architecture-image {
            width: 100%;
            margin: 20px 0;
            border-radius: 5px;
            box-shadow: 0 3px 10px rgba(0,0,0,0.1);
        }
        .bibtex {
            background-color: #f5f7fa;
            padding: 20px;
            border-radius: 5px;
            font-family: monospace;
            overflow-x: auto;
            margin-top: 20px;
        }
        footer {
            padding: 40px 0;
            background-color: #f5f7fa;
            text-align: center;
            color: #6c757d;
            margin-top: 60px;
        }
        .container {
            max-width: 1200px;
        }
        @media (max-width: 767.98px) {
            .header-section {
                padding: 70px 0 40px;
            }
            .paper-title {
                font-size: 1.8rem;
            }
            .content-section {
                padding: 40px 0;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light">
        <div class="container">
            <a class="navbar-brand" href="../index.html">Zhipeng Zhou</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html">Projects</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" href="../index.html">Publications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html">Competitions</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html">Activities</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html">Contact</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Header Section -->
    <header class="header-section">
        <div class="container">
            <h1 class="paper-title">Who Detects Better? A Comparative Study on Misinformation Detection by Humans and Large Language Models</h1>
            <div class="authors">
                <div class="author"><strong>Zhipeng Zhou</strong></div>
                <div class="author">Xiao Liu</div>
                <div class="author">Huaicheng He</div>
                <div class="author">Da Ding</div>
            </div>
            <div class="institution">
                Chongqing University<br>
                174 Shazheng St., Shapingba, Chongqing, 400044, China
            </div>
            <div class="authors mt-4">
                <div class="author">Qianshi Qi</div>
            </div>
            <div class="institution">
                Monash University<br>
                Melbourne, Australia
            </div>
            <div class="paper-links">
                <a href="#" class="paper-link">Project Page</a>
                <a href="cogsci-pdf.html" class="paper-link">PDF</a>
                <a href="https://github.com/anonymous-submission8888" class="paper-link">Code</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <section class="content-section">
        <div class="container">
            <h2 class="section-title">Abstract</h2>
            <div class="abstract">
                <p>
                    Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, their ability to detect and react to misinformation remains an open question, particularly in comparison to human cognitive mechanisms. This study investigates how LLMs and humans react to misinformation by analyzing their performance across five categories of errors: intellectual, common sense, reasoning, misleading, and logical errors. We construct the ErrorQuestionDataset, comprising 346 misinformation-related questions, and conduct an empirical study involving five state-of-the-art LLMs (ChatGPT-4o, Gemini-1.5-Flash, DeepSeek-v3, Hunyuan-Large, GLM-v4Flash) and 251 human participants. Our findings reveal distinct response patterns: while LLMs rely on statistical correlations and pattern recognition, humans leverage contextual reasoning and domain-specific knowledge. The results indicate that LLMs generally achieve higher accuracy than humans in error detection tasks, but their performances lack depth in reasoning-based assessments. Additionally, we identify five primary performance types—affirmation, negation, hesitation, questioning, and off-topic reactions—providing insights into the cognitive differences between LLMs and human cognition. Our study contributes to the broader understanding of misinformation detection and offers implications for enhancing the robustness and reliability of LLMs in real-world applications.
                </p>
            </div>

            <h2 class="section-title">Gallery</h2>
            <div class="gallery row">
                <div class="col-md-6">
                    <img src="../images/cogsci1.png" alt="Accuracy Comparison" class="gallery-image">
                    <p class="text-center mt-2">Accuracy Comparison across Models and Human Participants</p>
                </div>
                <div class="col-md-6">
                    <img src="../images/cogsci2.png" alt="Error Type Analysis" class="gallery-image">
                    <p class="text-center mt-2">Accuracy by Error Type: LLMs vs. Human Participants</p>
                </div>
            </div>

            <h2 class="section-title">Research Highlights</h2>
            <p>
                Our comparative study on misinformation detection explores several key aspects:
            </p>
            <ul>
                <li><strong>ErrorQuestionDataset:</strong> Construction of a dataset with 346 misinformation-related questions across five error categories</li>
                <li><strong>Human-AI Comparison:</strong> Analysis of how 5 state-of-the-art LLMs and 251 human participants detect and respond to misinformation</li>
                <li><strong>Category-Based Analysis:</strong> Detailed comparison across intellectual, common sense, reasoning, misleading, and logical error types</li>
                <li><strong>Response Pattern Identification:</strong> Classification of five primary performance types: affirmation, negation, hesitation, questioning, and off-topic reactions</li>
                <li><strong>Cognitive Mechanism Analysis:</strong> Exploration of the fundamental differences in how LLMs and humans process and respond to errors</li>
            </ul>

            <p>
                The research contributes to cognitive science by providing insights into the similarities and differences between artificial and human intelligence when confronted with misinformation, with implications for AI safety and reliability.
            </p>

            <h2 class="section-title">Implementation</h2>
            <p>
                Our study implementation includes:
            </p>
            <ol>
                <li><strong>Dataset Construction:</strong> Carefully designed questions across five error categories to test different aspects of error detection</li>
                <li><strong>LLM Evaluation Framework:</strong> Systematic testing methodology for assessing LLM responses to misinformation</li>
                <li><strong>Human Study Design:</strong> Controlled experiments with 251 participants to establish human baseline performance</li>
                <li><strong>Statistical Analysis:</strong> Rigorous comparative analysis of performance metrics between LLMs and humans</li>
                <li><strong>Response Classification:</strong> Framework for categorizing and analyzing different types of reactions to misinformation</li>
            </ol>
            <p>
                All experiment materials, code, and dataset are available on GitHub, allowing researchers to reproduce our findings and build upon this work for further research in misinformation detection.
            </p>

            <h2 class="section-title">BibTeX</h2>
            <div class="bibtex">
                <pre>@inproceedings{zhou2025who,
  title={Who Detects Better? A Comparative Study on Misinformation Detection by Humans and Large Language Models},
  author={Zhou, Zhipeng and Liu, Xiao and He, Huaicheng and Ding, Da and Qi, Qianshi},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  year={2025}
}</pre>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2024 Zhipeng Zhou. All rights reserved.</p>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html> 